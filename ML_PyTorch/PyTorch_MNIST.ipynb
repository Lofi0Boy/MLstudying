{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPX+Gz4Lp9DIRYw2bgUoEOn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Train Data-InputLayer-Hidden Layer - ReLU - Dropout - Output - SoftMax - Loss Function\n","\n","1. Train Data\n","28*28사이즈 이미지를 784개의 1차원 벡터로 변환\n","\\#flatten()\n","\n","2. Hidden Layer\n","number of HL & nodes are critical hyperparameters for training.\n","\n","3. ReLU\n","Kind of activation function.\n","If Input is larger than 0, the original value will be returned,\n","if not, it return 0.\n","\n","모델에 비선형성을 부여해줌\n","\n","4. Dropout\n","Function for decreasing the overfitting.\n","It removes several nodes of layer arbitrary following by given probability value.\n","\n","5. Output Layer\n","Number of Layer should be 10, that is same with number of answers.\n","\n","6.Softmax\n","It used when you make multi-class classification model.\n","it transfer the value of vector input between 0 to 1.\n","\n","**??**\n","ReLU의 비선형성이 왜 필요하지?\n","그리고 DropOut과 같은 과적합 방지책이 또 있나? 왜 이런식으로 방지를 하지?\n","\n","\n"],"metadata":{"id":"s4xZmfenUJFW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ak7uX3bvHWyf","executionInfo":{"status":"ok","timestamp":1708392308688,"user_tz":-540,"elapsed":24297,"user":{"displayName":"이상","userId":"14296519550561623483"}},"outputId":"3bccff20-9144-4069-e756-e112ea74ab45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import numpy as np\n","\n","\n","train_dataset = datasets.MNIST(root='MNIST_data/',train = True, transform=transforms.ToTensor(), download=True)\n","test_dataset = datasets.MNIST(root = 'MNIST_data/', train = False,transform=transforms.ToTensor(), download=True)\n","\n","print(len(train_dataset))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUfT8J-MHTwI","executionInfo":{"status":"ok","timestamp":1708418069075,"user_tz":-540,"elapsed":7853,"user":{"displayName":"이상","userId":"14296519550561623483"}},"outputId":"5b1e0f60-7c60-4775-e14f-e5c211eb7f14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 106252935.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 29007589.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 27135508.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 14779308.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n","\n","60000\n"]}]},{"cell_type":"code","source":["#train용, validation용 분류\n","#validation : train된 데이터가 과적합 되었는지 확인하기 위함\n","\n","train_dataset_size = int(len(train_dataset)*0.85)\n","validation_dataset_size = len(train_dataset)-train_dataset_size\n","train_dataset, validation_dataset = random_split(train_dataset,[train_dataset_size,validation_dataset_size])\n","\n","print(len(train_dataset),len(validation_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBSL7E8qHP-K","executionInfo":{"status":"ok","timestamp":1708418071988,"user_tz":-540,"elapsed":434,"user":{"displayName":"이상","userId":"14296519550561623483"}},"outputId":"d1d5c4e9-e704-4fe2-aeb9-099784a54444"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["51000 9000\n"]}]},{"cell_type":"code","source":["#한번에 트레이닝 시키는 묶음의 사이즈\n","BATCH_SIZE = 32\n","\n","train_dataset_loader = DataLoader(dataset=train_dataset,batch_size = BATCH_SIZE,shuffle = True)\n","validation_dataset_loader = DataLoader(dataset = validation_dataset,batch_size = BATCH_SIZE,shuffle =True)\n","test_dataset_laoder = DataLoader(dataset = validation_dataset,batch_size = BATCH_SIZE, shuffle = True)\n"],"metadata":{"id":"ePyUNzsLJapN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDeepLearningModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten = nn.Flatten()\n","    self.fc1 = nn.Linear(784,256) #input : 28*28=784, output:256 why? 알수없음.\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.Dropout(p=0.3)\n","    self.fc2 = nn.Linear(256,10)\n","\n","  def forward(self, data):\n","    data = self.flatten(data)\n","    data = self.fc1(data)\n","    data = self.relu(data)\n","    data = self.dropout(data)\n","    logits = self.fc2(data)\n","    return logits\n","\n","model = MyDeepLearningModel()\n","\n","#uhm, 확률적인 데이터에 CEL쓰더라\n","loss_function = nn.CrossEntropyLoss() #model - loss function - optimizer관계가 어떻게 되는건지.. loss function을 모델이나 optimizer에 마운트 시킨적이 없는데 어떻게 loss function값을 기반으로 model parameter를 업데이트 한다는거지???????????\n","\n","optimizer = torch.optim.SGD(model.parameters(),lr=1e-2)\n"],"metadata":{"id":"Mk3FXKDEKHHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model_train(dataloader,model,loss_function, optimizer):\n","  model.train()\n","  train_loss_sum = 0\n","  train_correct = 0\n","  train_total = 0\n","\n","  total_train_batch = len(dataloader)\n","\n","  for images, labels in dataloader:\n","\n","    #-1 : 다른 차원으로 reshape 함을 뜻하는듯\n","    x_train = images.view(-1,28*28)\n","    y_train = labels #정답 숫자\n","\n","    #output은 9개의 확률일텐데, 0~9숫자 자체인 y_train이랑 계산이 되나? CEL자체가 그런식으로 작동하는건가????????????????????????????????????????\n","    #nn.CrossEntropyLoss() 라는 손실함수 자체에 SoftMax알고리즘이 포함되어 있기때문에 가능..하다는건가\n","    #Softmax(소프트맥스)는 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수이다.\n","    #소프트맥스 결과값을 One hot encoder의 입력으로 연결하면 가장 큰 값만 True값, 나머지는 False값이 나오게 하여 이용 가능하다.\n","    output = model(x_train)\n","    loss = loss_function(output,y_train)\n","\n","    #이거 왜하는걸까?\n","    #1배치 끝내고나서 그래디언트 초기화해주는것 같긴한데 일단 패스!\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    train_loss_sum += loss.item()\n","\n","    train_total += y_train.size(0)\n","    #argmax(tensor,dimension), tensor.sum()==>tensor\n","    train_correct +=  ((torch.argmax(output,1)==y_train)).sum().item()\n","\n","  train_avg_loss = train_loss_sum / total_train_batch\n","  train_avg_accuracy = 100*train_correct/train_total\n","\n","  return (train_avg_loss, train_avg_accuracy)\n","\n","def model_evaluate(dataloader, model, loss_function, optimizer):\n","  #모델을 평가모드로 전환 ==> Dropout, Batchnorm등의 기능을 비활성화, 메모리랑 상관X\n","  #훈련모드는 model.train()\n","  model.eval()\n","\n","  with torch.no_grad(): #autogard engine비활성화, 메모리 leak 방지.\n","    val_loss_sum=0\n","    val_correct=0\n","    val_total =0\n","\n","    total_val_batch = len(dataloader)\n","\n","    for images, labels in dataloader :\n","      x_val = images.view(-1,28*28)\n","      y_val = labels\n","\n","      output = model(x_val)\n","      loss = loss_function(output, y_val)\n","\n","      val_loss_sum += loss.item()\n","\n","      val_total += y_val.size(0)\n","      val_correct += (torch.argmax(output,1)==y_val).sum().item()\n","\n","    val_avg_loss = val_loss_sum/total_val_batch\n","    val_avg_accuracy = 100*val_correct/val_total\n","\n","  return (val_avg_loss,val_avg_accuracy)\n","\n","def model_test(dataloader, model):\n","  model.eval()\n","\n","  with torch.no_grad():\n","    test_loss_sum = 0\n","    test_correct = 0\n","    test_total = 0\n","\n","    total_test_batch = len(dataloader)\n","    for images, labels in dataloader:\n","\n","      x_test = images.view(-1,28*28)\n","      y_test = labels\n","\n","      output = model(x_test)\n","      loss = loss_function(output, y_test)\n","\n","\n","      test_loss_sum += loss.item()\n","\n","      test_total += y_test.size(0)\n","      test_correct += (torch.argmax(output,1)==y_test).sum().item()\n","\n","    test_avg_loss = test_loss_sum / total_test_batch\n","    test_avg_accuracy = 100*test_correct / test_total\n","\n","    print('accuracy', test_avg_accuracy)\n","    print('loss', test_avg_loss)"],"metadata":{"id":"XDzppQbfJpKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","train_loss_list = []\n","train_accuracy_list = []\n","\n","val_loss_list=[]\n","val_accuracy_list = []\n","\n","start_time = datetime.now()\n","\n","EPOCHS = 20\n","\n","for epoch in range(EPOCHS):\n","\n","  #model training\n","  train_avg_loss, train_avg_accuracy = model_train(train_dataset_loader,model,loss_function, optimizer)\n","\n","  train_loss_list.append(train_avg_loss)\n","  train_accuracy_list.append(train_avg_accuracy)\n","\n","  #model evaluation\n","  val_avg_loss, val_avg_accuracy = model_evaluate(validation_dataset_loader,model, loss_function,optimizer)\n","\n","  val_loss_list.append(val_avg_loss)\n","  val_accuracy_list.append(val_avg_accuracy)\n","\n","  print('epoch:', '%02d' % (epoch + 1),\n","          'train loss =', '{:.4f}'.format(train_avg_loss), 'train accuracy =', '{:.4f}'.format(train_avg_accuracy),\n","          'validation loss =', '{:.4f}'.format(val_avg_loss), 'validation accuracy =', '{:.4f}'.format(val_avg_accuracy))\n","\n","end_time = datetime.now()\n","\n","print('elapsed time => ', end_time-start_time)"],"metadata":{"id":"AJSMRYdXPblP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708419509074,"user_tz":-540,"elapsed":221318,"user":{"displayName":"이상","userId":"14296519550561623483"}},"outputId":"895424c5-ebb5-4ca4-b519-35a13537d585"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 01 train loss = 0.0850 train accuracy = 97.5196 validation loss = 0.0910 validation accuracy = 97.3444\n","epoch: 02 train loss = 0.0839 train accuracy = 97.7000 validation loss = 0.0900 validation accuracy = 97.3556\n","epoch: 03 train loss = 0.0805 train accuracy = 97.7235 validation loss = 0.0895 validation accuracy = 97.3667\n","epoch: 04 train loss = 0.0805 train accuracy = 97.7118 validation loss = 0.0886 validation accuracy = 97.3667\n","epoch: 05 train loss = 0.0791 train accuracy = 97.7431 validation loss = 0.0885 validation accuracy = 97.4111\n","epoch: 06 train loss = 0.0774 train accuracy = 97.8020 validation loss = 0.0872 validation accuracy = 97.4667\n","epoch: 07 train loss = 0.0771 train accuracy = 97.7549 validation loss = 0.0866 validation accuracy = 97.4667\n","epoch: 08 train loss = 0.0739 train accuracy = 97.9196 validation loss = 0.0862 validation accuracy = 97.4222\n","epoch: 09 train loss = 0.0732 train accuracy = 97.8804 validation loss = 0.0850 validation accuracy = 97.5222\n","epoch: 10 train loss = 0.0729 train accuracy = 97.9176 validation loss = 0.0845 validation accuracy = 97.5000\n","epoch: 11 train loss = 0.0720 train accuracy = 97.9706 validation loss = 0.0835 validation accuracy = 97.5667\n","epoch: 12 train loss = 0.0717 train accuracy = 97.9373 validation loss = 0.0834 validation accuracy = 97.5556\n","epoch: 13 train loss = 0.0703 train accuracy = 98.0255 validation loss = 0.0831 validation accuracy = 97.5444\n","epoch: 14 train loss = 0.0689 train accuracy = 98.0686 validation loss = 0.0825 validation accuracy = 97.5444\n","epoch: 15 train loss = 0.0680 train accuracy = 98.0824 validation loss = 0.0816 validation accuracy = 97.6000\n","epoch: 16 train loss = 0.0664 train accuracy = 98.1314 validation loss = 0.0808 validation accuracy = 97.6556\n","epoch: 17 train loss = 0.0667 train accuracy = 98.0765 validation loss = 0.0805 validation accuracy = 97.6556\n","epoch: 18 train loss = 0.0648 train accuracy = 98.2216 validation loss = 0.0800 validation accuracy = 97.6000\n","epoch: 19 train loss = 0.0646 train accuracy = 98.1824 validation loss = 0.0790 validation accuracy = 97.6778\n","epoch: 20 train loss = 0.0625 train accuracy = 98.2667 validation loss = 0.0793 validation accuracy = 97.5778\n","elapsed time =>  0:03:41.062430\n"]}]}]}