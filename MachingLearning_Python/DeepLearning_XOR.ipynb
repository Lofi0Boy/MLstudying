{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3VaMSlhX8SVH9QozCL+Oo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Deep Learning**\n","\n","\n"],"metadata":{"id":"c7SEd90gYTOz"}},{"cell_type":"code","source":["import numpy as np\n","\n","def numerical_derivative(f, x):\n","  delta = 1e-4\n","\n","  grad = np.zeros_like(x)\n","\n","  itr = np.nditer(x,flags=['multi_index'])\n","\n","  while(not itr.finished):\n","    idx = itr.multi_index\n","\n","    ord = x[idx]\n","    x[idx]=x[idx]+delta\n","    fx1 = f(x)\n","\n","    x[idx]=ord-delta\n","    fx2 = f(x)\n","\n","    grad[idx] = (fx1-fx2)/(delta*2)\n","    x[idx]=ord\n","\n","    itr.iternext()\n","  #grad는 x와 같은 차원으로 리턴될 것.\n","  return grad\n","\n","def sigmoid(x):\n","  return 1 / (1+np.exp(-x))\n","\n"],"metadata":{"id":"1GoXbBA1Yoj2","executionInfo":{"status":"ok","timestamp":1706676857463,"user_tz":-540,"elapsed":251,"user":{"displayName":"이상","userId":"14296519550561623483"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class LogicGate:\n","\n","    def __init__(self, gate_name, xdata, tdata):\n","\n","        self.name = gate_name\n","        self.__xdata = xdata.reshape(4,2)\n","        self.__tdata = tdata.reshape(4,1)\n","\n","        self.__w2 = np.random.rand(2,6)\n","        self.__b2 = np.random.rand(6)\n","\n","        self.__w3 = np.random.rand(6,1)\n","        self.__b3 = np.random.rand(1)\n","\n","\n","        self.__learning_rate = 1e-2\n","\n","\n","    def feed_forward(self):        # feed forward 를 통하여 손실함수(cross-entropy) 값 계산\n","\n","        delta = 1e-7\n","\n","        z2 = np.dot(self.__xdata,self.__w2)+self.__b2\n","        a2 = sigmoid(z2)\n","\n","        z3 = np.dot(a2,self.__w3)+self.__b3\n","        y = a3 = sigmoid(z3)\n","\n","        return  -np.sum( self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1 - y)+delta ) )\n","\n","\n","    def loss_val(self):          # 외부 출력을 위한 손실함수(cross-entropy) 값 계산\n","\n","        delta = 1e-7    # log 무한대 발산 방지\n","\n","        z2 = np.dot(self.__xdata, self.__w2) + self.__b2  # 은닉층의 선형회귀 값\n","        a2 = sigmoid(z2)                                  # 은닉층의 출력\n","\n","        z3 = np.dot(a2, self.__w3) + self.__b3            # 출력층의 선형회귀 값\n","        y = a3 = sigmoid(z3)                              # 출력층의 출력\n","\n","        # cross-entropy\n","        return  -np.sum( self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1 - y)+delta ) )\n","\n","\n","\n","\n","    # 수치미분을 이용하여 손실함수가 최소가 될때 까지 학습하는 함수\n","    def train(self):\n","\n","        f = lambda x : self.feed_forward()\n","\n","        print(\"Initial loss value = \", self.loss_val())\n","\n","        for step in  range(10001):\n","\n","            self.__w2 -= self.__learning_rate * numerical_derivative(f, self.__w2)\n","\n","            self.__b2 -= self.__learning_rate * numerical_derivative(f, self.__b2)\n","\n","            self.__w3 -= self.__learning_rate * numerical_derivative(f, self.__w3)\n","\n","            self.__b3 -= self.__learning_rate * numerical_derivative(f, self.__b3)\n","\n","            if (step % 400 == 0):\n","                print(\"step = \", step, \"  , loss value = \", self.loss_val())\n","\n","\n","\n","\n","    # query, 즉 미래 값 예측 함수\n","    def predict(self, xdata):\n","\n","        z2 = np.dot(xdata, self.__w2) + self.__b2         # 은닉층의 선형회귀 값\n","        a2 = sigmoid(z2)                                  # 은닉층의 출력\n","\n","        z3 = np.dot(a2, self.__w3) + self.__b3            # 출력층의 선형회귀 값\n","        y = a3 = sigmoid(z3)                              # 출력층의 출력\n","\n","        if y > 0.5:\n","            result = 1  # True\n","        else:\n","            result = 0  # False\n","\n","        return y, result\n","\n"],"metadata":{"id":"zDg4_b9ZZPF3","executionInfo":{"status":"ok","timestamp":1706676838130,"user_tz":-540,"elapsed":265,"user":{"displayName":"이상","userId":"14296519550561623483"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n","tdata = np.array([0, 1, 1, 0])\n","\n","\n","xor_obj = LogicGate(\"XOR\", xdata, tdata)\n","\n","xor_obj.train()\n","\n","test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n","\n","for data in test_data:\n","    print(xor_obj.predict(data))"],"metadata":{"id":"Hx09pPfoYoma","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706676884314,"user_tz":-540,"elapsed":23247,"user":{"displayName":"이상","userId":"14296519550561623483"}},"outputId":"28e1a5ca-f437-42c9-ecbf-2447a0f18bfd"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial loss value =  4.574508280606054\n","step =  0   , loss value =  4.483881267203683\n","step =  400   , loss value =  2.7740228119748345\n","step =  800   , loss value =  2.7718849314759524\n","step =  1200   , loss value =  2.7697165392351577\n","step =  1600   , loss value =  2.7673261811881202\n","step =  2000   , loss value =  2.764491469304783\n","step =  2400   , loss value =  2.760922325699202\n","step =  2800   , loss value =  2.756210420017754\n","step =  3200   , loss value =  2.7497561138470443\n","step =  3600   , loss value =  2.7406651240051856\n","step =  4000   , loss value =  2.727613677540308\n","step =  4400   , loss value =  2.7086927167566706\n","step =  4800   , loss value =  2.681239288700148\n","step =  5200   , loss value =  2.6416162540943837\n","step =  5600   , loss value =  2.584869080107495\n","step =  6000   , loss value =  2.504394547309698\n","step =  6400   , loss value =  2.3922673326682036\n","step =  6800   , loss value =  2.241082694844825\n","step =  7200   , loss value =  2.0473932264403034\n","step =  7600   , loss value =  1.8160274526876166\n","step =  8000   , loss value =  1.563499106175879\n","step =  8400   , loss value =  1.3148790321787103\n","step =  8800   , loss value =  1.0924363497390053\n","step =  9200   , loss value =  0.9068673611933695\n","step =  9600   , loss value =  0.7581527994841205\n","step =  10000   , loss value =  0.640924369542192\n","(array([0.05512611]), 0)\n","(array([0.8554485]), 1)\n","(array([0.8406014]), 1)\n","(array([0.22466024]), 0)\n"]}]}]}